{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/owenoertell/Downloads/dataset\n"
     ]
    }
   ],
   "source": [
    "cd ~/Downloads/dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed libraries.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3833 files belonging to 2 classes.\n",
      "Using 3067 files for training.\n",
      "Found 3833 files belonging to 2 classes.\n",
      "Using 766 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# Setting hyperparameters. In this case the image height and width are used to make the data more uniform.\n",
    "# eventually do need resize image to then pass to the model in practice. \n",
    "\n",
    "batch_size = 32\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "\n",
    "# Import that training dataset. This dataset consisted of a total of 3833 images. Used the standard 20% for\n",
    "# validation and 80% for training.\n",
    "\n",
    "trainDataSet = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"/home/owenoertell/Downloads/dataset/\", labels='inferred', label_mode='categorical',\n",
    "    color_mode='grayscale', batch_size=32, image_size=(img_width, img_height), shuffle=True, seed=4236,\n",
    "    validation_split=0.2, subset=\"training\", interpolation='bilinear')\n",
    "\n",
    "validationDataSet = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"/home/owenoertell/Downloads/dataset/\", labels='inferred', label_mode='categorical',\n",
    "    color_mode='grayscale', batch_size=32, image_size=(img_width, img_height), shuffle=True, seed=4236,\n",
    "    validation_split=0.2, subset=\"validation\", interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model.\n",
    "\n",
    "model = Sequential([\n",
    "  # The first layer is a rescaling to normalize the data. By ensuring that all of the values are approxomately\n",
    "  # similar this makes the weights more effective.\n",
    "  layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 1)),\n",
    "  # Convolution layers. First argument is the number of kernals for the covolution to learn,\n",
    "  # second is the size of the convolution window, padding is for if the convolution window runs over.\n",
    "  # Activation function is used to remove a linear association. The rectify linear (very common function) was used.\n",
    "  # the relu(x) = max(0,x)\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  # Reduces the sampling size and \n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  # Flatten fixes the shape to be passed to dense node layer. \n",
    "  layers.Flatten(),\n",
    "  # Apply dropout. This means that the rate 0.6 the input will become 0. This is used to prevent overfitting\n",
    "  # of the model to the data.\n",
    "  layers.Dropout(0.6),\n",
    "  # Passed to dense layer with 128 nodes. Uses relu activation function again\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  # Passed to the prediction layer. Each node represents a class. In this case one represents mask and the other\n",
    "  # not having a mask.\n",
    "  layers.Dense(2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  # Uses the adam optimizer. While could have manually tuned hyperparameters, the adam optamiser is usually\n",
    "  # sufficent and in this case it was.\n",
    "  optimizer='adam',\n",
    "  # Specify the way loss is measured. Categorical crossentropy quantifies the error of a prediction during\n",
    "  # training.\n",
    "  loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n",
    "  # Metrics to measure for each epoch. Used to look for instances of overfitting and/or if the model is\n",
    "  # approaching optimal accuracy.\n",
    "  metrics=['accuracy', 'mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "96/96 [==============================] - 84s 873ms/step - loss: 0.4853 - accuracy: 0.7590 - mean_squared_error: 0.8730 - val_loss: 0.3903 - val_accuracy: 0.8003 - val_mean_squared_error: 1.3686\n",
      "Epoch 2/10\n",
      "96/96 [==============================] - 83s 865ms/step - loss: 0.3236 - accuracy: 0.8556 - mean_squared_error: 2.0643 - val_loss: 0.3584 - val_accuracy: 0.8316 - val_mean_squared_error: 3.2056\n",
      "Epoch 3/10\n",
      "96/96 [==============================] - 62s 644ms/step - loss: 0.2256 - accuracy: 0.9077 - mean_squared_error: 4.4964 - val_loss: 0.2859 - val_accuracy: 0.8930 - val_mean_squared_error: 8.9658\n",
      "Epoch 4/10\n",
      "96/96 [==============================] - 57s 591ms/step - loss: 0.1684 - accuracy: 0.9273 - mean_squared_error: 7.4124 - val_loss: 0.2817 - val_accuracy: 0.8982 - val_mean_squared_error: 12.7934\n",
      "Epoch 5/10\n",
      "96/96 [==============================] - 56s 583ms/step - loss: 0.1292 - accuracy: 0.9547 - mean_squared_error: 11.7158 - val_loss: 0.2425 - val_accuracy: 0.9282 - val_mean_squared_error: 19.1252\n",
      "Epoch 6/10\n",
      "96/96 [==============================] - 58s 608ms/step - loss: 0.0912 - accuracy: 0.9684 - mean_squared_error: 16.4589 - val_loss: 0.2415 - val_accuracy: 0.9243 - val_mean_squared_error: 17.9791\n",
      "Epoch 7/10\n",
      "96/96 [==============================] - 59s 619ms/step - loss: 0.0714 - accuracy: 0.9736 - mean_squared_error: 20.8646 - val_loss: 0.2655 - val_accuracy: 0.9347 - val_mean_squared_error: 27.5523\n",
      "Epoch 8/10\n",
      "96/96 [==============================] - 60s 623ms/step - loss: 0.0661 - accuracy: 0.9729 - mean_squared_error: 31.0152 - val_loss: 0.3365 - val_accuracy: 0.9334 - val_mean_squared_error: 32.2404\n",
      "Epoch 9/10\n",
      "96/96 [==============================] - 58s 607ms/step - loss: 0.0567 - accuracy: 0.9788 - mean_squared_error: 33.8642 - val_loss: 0.3280 - val_accuracy: 0.9373 - val_mean_squared_error: 42.8518\n",
      "Epoch 10/10\n",
      "96/96 [==============================] - 59s 613ms/step - loss: 0.0476 - accuracy: 0.9830 - mean_squared_error: 42.2799 - val_loss: 0.3178 - val_accuracy: 0.9373 - val_mean_squared_error: 54.3008\n"
     ]
    }
   ],
   "source": [
    "# Train the model. Pass in training and validation data sets along with the number of epochs.\n",
    "# 10 epochs was chosen to allow the model to be quite accurate but not overfit.\n",
    "modelOutput = model.fit(\n",
    "  trainDataSet,\n",
    "  validation_data=validationDataSet,\n",
    "  epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot model accuracy during training. The final and maximum validation accuracy was 0.9373\n",
    "\n",
    "hist = pd.DataFrame(modelOutput.history)\n",
    "plt.plot(hist['accuracy'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# While this code was not used in the final iteration, it captures and reads input.\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "# Capture camera input\n",
    "cap = cv.VideoCapture(0)\n",
    "ret, frame = cap.read()\n",
    "grayScaleValue = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "cap.release()\n",
    "\n",
    "# Create haar and read face.\n",
    "\n",
    "# Load model for face detection\n",
    "faceCascade = cv.CascadeClassifier('/home/owenoertell/Downloads/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# dectect faces. One signficant blocker we ran into was that facial detection with a mask was quite difficult.\n",
    "# which led us to needing to reduce the threshold for facial detection.\n",
    "faces = faceCascade.detectMultiScale(\n",
    "        grayScaleValue,\n",
    "        scaleFactor=1.02,\n",
    "        minNeighbors=5,\n",
    "        minSize=(10, 10)\n",
    "    )\n",
    "# Splice the orignal picture to include only the face.\n",
    "for (x,y,w,h) in faces:\n",
    "    #cv.rectangle(grayScaleValue,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    face = grayScaleValue[y:y+h, x:x+w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the face array. Add dimensions to allow for preprocessing by keras.\n",
    "newFace = np.expand_dims(face, axis=0)\n",
    "\n",
    "faceAsImg = tf.keras.preprocessing.image.array_to_img(newFace, data_format='channels_first')\n",
    "# resize image to fit 256x256 as set by hyperparameters.\n",
    "newFaceAsImage = faceAsImg.resize([256,256],1)\n",
    "\n",
    "# Convert back to array and display image.\n",
    "imgAsArray = tf.keras.preprocessing.image.img_to_array(newFaceAsImage, data_format='channels_first')\n",
    "newFaceAsImage.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-2e3942015df9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Use the model to make a prediction on the taken image.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgAsArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Use softmax function to get a score.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Use the model to make a prediction on the taken image.\n",
    "predictions = model.predict(imgAsArray)\n",
    "\n",
    "# Use softmax function to get a score.\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "class_names = [\"yes mask\", \"no mask\"]\n",
    "# Using the argmax value, report which class to which the image most likely belongs.\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
